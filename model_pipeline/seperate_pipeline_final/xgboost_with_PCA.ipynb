{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from typing import List, Optional\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import ParameterGrid as PG\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntest_split_data_by_submarket(data, ntest):\n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "    submarkets = data['research_submkt_id'].unique()\n",
    "\n",
    "    for submarket in submarkets:\n",
    "        submarket_data = data[data['research_submkt_id'] == submarket]\n",
    "        train_submarket = submarket_data.iloc[:-ntest]\n",
    "        test_submarket = submarket_data.iloc[-ntest:]\n",
    "        train_data = pd.concat([train_data, train_submarket])\n",
    "        test_data = pd.concat([test_data, test_submarket])\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "\n",
    "def all_get_submkt_forecast(df, num_lags, ntest, feature_subset):\n",
    "    df_new = df[['date', 'research_submkt_id', 'real_hedonic_rent_submarket']]\n",
    "    df_new['date'] = pd.to_datetime(df_new['date'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    if num_lags is None:\n",
    "        num_lags = 36\n",
    "    \n",
    "    num_lags = 36  \n",
    "    for lag in range(1, num_lags+1):\n",
    "        df_new['rent_{}months_ago'.format(lag)] = df_new.groupby('research_submkt_id')['real_hedonic_rent_submarket'].shift(lag)\n",
    "    df_new = df_new.dropna()\n",
    "    df_new = df_new.sort_values(['date', 'research_submkt_id']).reset_index(drop=True)\n",
    "    \n",
    "    features = [\n",
    "             'total_property_sqft',\n",
    "             'gdp_histfc',\n",
    "             'population_histfc',\n",
    "             'nominal_retail_sales_histfc',\n",
    "             'employment_warehousing_histfc',\n",
    "             'income_per_capita_histfc',\n",
    "             'nominal_proprietors_income_histfc',\n",
    "             'housing_completions_histfc',\n",
    "             'real_ecommerce',\n",
    "             'imports_us',\n",
    "             'spread_3m10y', \n",
    "             'ecomm^2_pop',\n",
    "             'real_market_level_rent',\n",
    "             'weighted_pop_estimate_cryr',\n",
    "             'weighted_hh_estimate_cryr',\n",
    "             'total_dock_doors',\n",
    "             'total_car_spaces',\n",
    "             'dock_door_ratio',\n",
    "             'number_of_car_spaces_ratio']\n",
    "\n",
    "    data = df[['date', 'research_submkt_id'] + features].values\n",
    "    date_and_id = data[:, :2]\n",
    "    features_data = data[:, 2:]\n",
    "\n",
    "    features_data = features_data.astype(float)\n",
    "    features_standardized = (features_data - features_data.mean(axis=0)) / features_data.std(axis=0)\n",
    "    pca = PCA()\n",
    "    pca.fit(features_standardized)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    n_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "    principal_components = pca.components_[:n_components]\n",
    "    features_transformed = features_standardized.dot(principal_components.T)\n",
    "    data_transformed = np.concatenate((date_and_id, features_transformed), axis=1)\n",
    "    \n",
    "    columns = ['date', 'research_submkt_id'] + [f'PC{i+1}' for i in range(n_components)]\n",
    "    df_sel = pd.DataFrame(data_transformed, columns=columns)\n",
    "    df_sel['date'] = data['date']\n",
    "    df_sel['research_submkt_id'] = data['research_submkt_id']\n",
    "    df_new = df_new.merge(df_sel, on=['date', 'research_submkt_id'], how='left')\n",
    "    \n",
    "    X = df_new.iloc[:, [0, 1] + list(range(3, len(df_new.columns)))]\n",
    "    Y = df_new.iloc[:, :3]\n",
    "\n",
    "    Y_train, Y_test = ntest_split_data_by_submarket(Y, ntest)\n",
    "    y_train = Y_train.iloc[:, -1]\n",
    "    y_test = Y_test.iloc[:, -1]\n",
    "    X_train, X_test = ntest_split_data_by_submarket(X, ntest)\n",
    "    x_train = X_train.iloc[:, 2:]\n",
    "    x_test = X_test.iloc[:, 2:]\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.1, 0.01, 0.001]\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor()\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Get the best parameters and best score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = -grid_search.best_score_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Score (RMSE):\", best_score)\n",
    "    \n",
    "\n",
    "    best_model = XGBRegressor(**best_params)\n",
    "    best_model.fit(x_train, y_train)\n",
    "\n",
    "    importance_scores = best_model.feature_importances_\n",
    "    attribute_names = x_train.columns\n",
    "    attribute_importance_dict = dict(zip(attribute_names, importance_scores))\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    Y_test_pred = Y_test.copy()\n",
    "    Y_test_pred['y_pred'] = y_pred\n",
    "\n",
    "    whole_smape = smape(y_test, y_pred)\n",
    "\n",
    "    smape_dic = {}\n",
    "    submkt_id = df_new['research_submkt_id'].unique().tolist()\n",
    "    for submkt in submkt_id:\n",
    "        y_submkt_test = Y_test_pred[Y_test_pred['research_submkt_id'] == submkt]['real_hedonic_rent_submarket']\n",
    "        y_submkt_pred = Y_test_pred[Y_test_pred['research_submkt_id'] == submkt]['y_pred']\n",
    "        submkt_smape = smape(y_submkt_test, y_submkt_pred)\n",
    "        smape_dic[submkt] = submkt_smape\n",
    "\n",
    "    # print(f\"{submkt} - SMAPE: {smape_value:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'Y_test_pred': Y_test_pred,\n",
    "        'smape_dic': smape_dic,\n",
    "        'whole_smape': whole_smape,\n",
    "        'best_params': best_params,\n",
    "        'attribute_importance': attribute_importance_dict\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_feature_subsets(\n",
    "        feature_space,\n",
    "        subset_size,\n",
    "        include_features: Optional[List[str]] = None,\n",
    "        intersect_size: int = 1,\n",
    "):\n",
    "    subset_size = max(1, subset_size)\n",
    "    subset_size = min(subset_size, len(feature_space))\n",
    "    subset_li = []\n",
    "\n",
    "    for subset in itertools.combinations(feature_space, subset_size):\n",
    "        subset = list(subset)\n",
    "        if include_features is not None:\n",
    "            intersect_size = min(subset_size, intersect_size)\n",
    "        subset_li.append(subset)\n",
    "\n",
    "    return subset_li\n",
    "\n",
    "\n",
    "\n",
    "def run_attribute_permutation_pipeline(df, num_lags, ntest, feature_space, subset_size, intersect_size):\n",
    "\n",
    "    feature_subsets = get_feature_subsets(\n",
    "        feature_space,\n",
    "        subset_size=subset_size,\n",
    "        include_features=None,\n",
    "        intersect_size=intersect_size\n",
    "    )\n",
    "    \n",
    "    param_vals = {\n",
    "        \"feature_subsets\": feature_subsets\n",
    "    }\n",
    "    param_grid = list(PG(param_vals))\n",
    "    num_params = len(param_grid)\n",
    "    \n",
    "    results = {}\n",
    "    pool = mp.Pool(processes=mp.cpu_count())\n",
    "\n",
    "    for idx, params in enumerate(param_grid):\n",
    "        print(f\"training model {idx}/{num_params - 1}: {params}\")\n",
    "        result = pool.apply_async(all_get_submkt_forecast, kwds={\n",
    "            \"df\": df,\n",
    "            \"num_lags\": num_lags,\n",
    "            \"ntest\": ntest,\n",
    "            \"feature_subset\": params['feature_subsets'],\n",
    "            \n",
    "        })\n",
    "        params = str(params)\n",
    "        results[params] = result\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    df_parts = []\n",
    "\n",
    "    # Iterate over each key-value pair in 'results'\n",
    "    for key, value in results.items():\n",
    "        dic = value.get()\n",
    "        y_pred_df = dic['Y_test_pred']\n",
    "        smape_dic = dic['smape_dic']\n",
    "        best_params = dic['best_params']\n",
    "        attribute_importance = dic['attribute_importance']\n",
    "        data = []\n",
    "\n",
    "        for index, row in y_pred_df.iterrows():\n",
    "            research_submkt_id = row['research_submkt_id']\n",
    "            date = row['date']\n",
    "            real_hedonic_rent_submarket = row['real_hedonic_rent_submarket']\n",
    "            y_pred = row['y_pred']\n",
    "\n",
    "            smape = smape_dic.get(research_submkt_id)\n",
    "\n",
    "            submarket_best_params = best_params\n",
    "            \n",
    "            submarket_attribute_importance = attribute_importance\n",
    "\n",
    "            submarket_info = {\n",
    "                'research_submkt_id': research_submkt_id,\n",
    "                'date': date,\n",
    "                'real_hedonic_rent_submarket': real_hedonic_rent_submarket,\n",
    "                'y_pred': y_pred,\n",
    "                'smape': smape,\n",
    "                'best_hyperparams': submarket_best_params,\n",
    "                'best_attributes': key,\n",
    "                'attribute_importance': submarket_attribute_importance\n",
    "            }\n",
    "\n",
    "            data.append(submarket_info)\n",
    "\n",
    "        df_part = pd.DataFrame(data)\n",
    "\n",
    "        df_parts.append(df_part)\n",
    "\n",
    "    df = pd.concat(df_parts, ignore_index=True)\n",
    "    min_smape_index = df.groupby('research_submkt_id')['smape'].idxmin()\n",
    "    smallest_smape_df = df.loc[min_smape_index, ['research_submkt_id', 'smape', 'best_hyperparams', 'best_attributes','attribute_importance']]\n",
    "\n",
    "\n",
    "    return df, smallest_smape_df\n",
    "\n",
    "\n",
    "def plot_submkt_forecast(df, smallest_smape_df):\n",
    "    for index, row in smallest_smape_df.iterrows():\n",
    "        research_submkt_id = row['research_submkt_id']\n",
    "        group = row['best_attributes']\n",
    "        smape = row['smape']\n",
    "\n",
    "        # Filter 'df' based on 'research_submkt_id' and 'group'-best_attributes\n",
    "        submarket_df = df[(df['research_submkt_id'] == research_submkt_id) & (df['best_attributes'] == group)]\n",
    "\n",
    "        # Extract the test and predicted values\n",
    "        dates = submarket_df['date']\n",
    "        y_test = submarket_df['real_hedonic_rent_submarket']\n",
    "        y_pred = submarket_df['y_pred']\n",
    "\n",
    "        # Plot the test and predicted values\n",
    "        plt.plot(dates, y_test, label='Test')\n",
    "        plt.plot(dates, y_pred, label='Predicted')\n",
    "        #plt.yticks([4.5,4.75,5.0,5.25,5.5,5.75,6.0], ['4.5','4.75','5.0','5.25','5.5','5.75','6.0'])\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Rent')\n",
    "        plt.title(f'Submarket {research_submkt_id} - Group {group} (SMAPE: {smape:.2f})')  # Include SMAPE in the plot title\n",
    "        plt.legend()\n",
    "\n",
    "        # Add SMAPE as a text annotation\n",
    "        plt.annotate(f'SMAPE: {smape:.2f}', xy=(0.02, 0.92), xycoords='axes fraction')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1088b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/container1/zqiao_Workspace/link-research/ad-hoc/zq-sandbox/submkt_data/submkt_train_data/pho_submkt_train_test_data.csv')\n",
    "num_lags = 36\n",
    "ntest = 24\n",
    "subset_size = 2\n",
    "intersect_size = 1\n",
    "feature_space = [\n",
    "                 \"real_market_level_rent\",\n",
    "                 \"gdp_histfc\",\n",
    "                 \"nominal_retail_sales_histfc\",\n",
    "              ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df, smallest_smape_df = run_attribute_permutation_pipeline(df, num_lags, ntest, feature_space, subset_size, intersect_size)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586233a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6526c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
